======================== SCHEDULER ==========================

example run:

./sched -t 4 -f sched.txt -b 20M -q 100 -i 20 -c 5 -- -flowsets 100:1500:1,1:1500:16 -alg rr

Usage:

./sched [client options] [-- [scheduler options]]

Client options:
    -t	threads		number of client threads
    -f	path		client configuration file (default: 'sched.txt')
    -q  size		size of queue between each client and the scheduler
    -b  bw		output bandwidth (in bps)
    -i  time		scheduler polling interval (in us)
    -c  time		client polling interval (in us)
    -p	PREFIX		timestap files will be called PREFIXqueue-i.txt
			(default: "")

Scheduler options:	...


Configuration file syntax:

    Each client can be programmed to generate traffic according
    to a sequence of specifications. Each specification consists
    in a time duration, a bandwidth, a packet size and a burst.
    Each specification is given on a single line, with values
    given in order and separated by spaces. E.g.,

	1s	100M	64  1

    will cause the thread to generate 64b packets at 100Mbps 
    for 1s, in bursts of 1 packet.

    Specifications are appended to the program of the current
    thread, which is thred number 0 at the beginning of the
    parsing.
    Lines containing the words 'thread n', where n >= 0,
    switch the current thread to thread number n.

    Specification destined to a non existing thread are ignored.

    Lines beginning with '#' are ignored.
    

Timestap files
    
    These are created at the end of the execution, one for each client thread.
    Each line contains a TAB separated list of fields, each related to a
    single packet sent through the queue. The fields are as follows:
	
    cli_enq:    absolute timestamp taken when the client enqueued the packet
    sch_enq:	clocks elapsed between cli_enq and the time when the scheduler
		has enqueued the packet into its own queues
    sch_deq:	clocks elapsed between sch_enq and the time when the scheduler
		has marked the packet as eligible for transmission
    cli_deq:	clocks elapsed between sch_deq and the time when the client
		has seen the mark and transmitted the packet
    pkt_len:	lenght of the packet in bytes
    flags:	either 'D' or nothing. 'D' means that the packet was actually
		dropped while the scheduler was trying to enqueue it.

EXPERIMENTS ON NODE15

A:	200G, 1500, 1, -b 500G -q 500 -i 5 -alg rr
	clients can do only 15Mpps, so -i 10 is enough with 1 client
B:	200G, 1500, 10, -b 500G -q 500 -i 5 -alg rr
C:	200G, 1500, 50, -b 500G -q 500 -i 5 -alg rr
D:	200G, 1500, 200, -b 500G -q 500 -i 5 -alg rr
E:	200G, 1500, 1000, -b 500G -q 500 -i 5 -alg rr
F:	500G, 1500, 100, -b 500G -q 14000 -i 20 -alg rr new code cache
F:	500G, 1500, 1000, -b 500G -q 14000 -i 20 -alg rr new code cache

Clients		aggregate Mpps

		A	B	C	D	E	F	G
1		15.2	19.8			20.9	21.2	21.3
2		11.9	18.6
3		10.9
4		11.5
5		11.7	17.2			19.0	19.2	19.1
6		8.2	10.3			17.0	17.3	17.3
7		6.9	10.3
8
9
10
11		7.4	10.3			16.8	16.2
12		6.7	 8.6			11.8	11.45
13		5.8
14
15
16
17		5.8	7.4
18
19
20
21
22
23		3.86	4.2	4.9	7.2	9.3	6.02	9.2

--- 20160408 20.53
./sched -a 22 -t 1 -b 10T -q 20 -- -flowsets 1:1500:32 -alg rr

Qsize	CLI	norm	A2	A3	A4
20	1	2.96	1.67	1.75
100	1	5.01	6.80	6.90
200	1	4.54	10.90	10.98
1000	1	6.54	19.37
2000	1	7.13

20	6	10.51	7.29	7.40
100			16.24
200			20.5
1000	6	16.62
2000

20	22	14.47	12.22	12.07
100			21.33
200
1000	22	19.86	20.07
2000


================== TRYLOCK TEST PROGRAM ==================

The trylock test program is used to measure the cost of POSIX semaphores
when used for inter-process locking with one or more processes.

Two types of tests are available:
    [A] Contention on a lock which is always busy. We call trylock in
        a tight loop and we measure how many operations per second we
        are able to do.
    [B] Contention on a lock which which is initially free, so that the
        processes pass the lock token around. We use a clock to sample
        the cost of trylock and unlock operations, and compute an
        average.
        For trylock operations, we check whether the lock was free or
        busy, and separate the measurements, since the cost is different.
        When we find the lock busy, we also collect the samples in a
        large circular array, so that we can later analyze the cost
        distribution. This is important because of some lock implementation
        details: a trylock on a busy lock may access a CPU-local cache (hit)
        or the cache of a remote CPU (miss) and/or main memory. Therefore,
        the average does not give information about the worst case, and
        an analysis of the distribution is necessary.

The implementation uses pthread threads in place of O.S. processes, and
each thread is pinned to a different core.

Usage:

./trylock [options]

Options:
    -n          number of threads to use
    -x          identifier of the core which is used to allocate the
                lock (used to see if this has some local effects)
    -t          number of trylock operations that each thread should
                perform, in millions
    -F          carry out test B (if not set, test A is carried out)
    -o          only for test B, it specifies a filename where we dump
                cost samples of trylock when the lock
                is found busy

Examples:

$ ./trylock -n 3     # three threads, busy lock
001.843513 fn_busy    [109] #00 busy: 2.031e+08 samples 2.410e+08 ops, free: 0.000000e+00 samples 0.000e+00 ops
001.843514 fn_busy    [109] #01 busy: 2.034e+08 samples 2.412e+08 ops, free: 0.000000e+00 samples 0.000e+00 ops
001.843514 fn_busy    [109] #02 busy: 2.024e+08 samples 2.413e+08 ops, free: 0.000000e+00 samples 0.000e+00 ops
001.843564 main       [278] BUSY: total 7.235e+08 ops, per-thread 2.412e+08 ops

    The output reports a total of 723 millions trylocks per second, which
    corresponds to 241 millions for each thread.

$ ./trylock -n 3 -F -o dump.txt  # three threads, free lock
004.887652 fn_free    [173] #00 busy: 2.749e+07 samples 5.700e+01 ns, free: 1.258e+07 samples 4.500e+01 ns, post: 2.570e+02 ns, duration 3.886578 s
004.887735 fn_free    [173] #02 busy: 4.703e+07 samples 5.500e+01 ns, free: 3.978e+06 samples 5.400e+01 ns, post: 2.561e+02 ns, duration 3.884700 s
004.887752 fn_free    [173] #01 busy: 3.568e+07 samples 5.400e+01 ns, free: 9.536e+06 samples 4.500e+01 ns, post: 2.547e+02 ns, duration 3.886993 s
004.887848 main       [290] t_busy 5.533e+01 ns, t_free 4.800e+01 ns

    The output reports an average of 55 nanoseconds for trylock cost then
    lock was busy and an average 48 nanosecond when the lock was free. The
    dump.txt file contains cost samples of busy lock (one per line) for all
    threads, to be used for statistical analysis.


================== NOTIFY TEST PROGRAM ==================

The notify test program is used to measure the cost of inter-process
notifications and wake-up costs, using a Linux eventfd mechanism
or a Unix pipe.

Two types of tests are available:
    [A] Blocking and wake-up cost for the notification receiver.
        We issue a (blocking) wait() operation inside a tight loop and we
        measure how many operations per second we are able to do.
    [B] Cost for the notification sender when the receiver is sleeping,
        waiting for a notification. This is the worst case cost for the
        sender, since it needs to do (in-kernel) work in order to wake up
        the peer. The sender uses a clock to sample the cost, and waits
        some time (e.g. 30 us) between two notifications, in order to
        give enough time to the receiver to go to sleep. An average is
        then computed over the samples.

The implementation uses two pthread threads in place of O.S. processes.

Usage:

./trylock [options]

Options:
    -n          number of notifications to be sent, in thousands
    -s          identifier of the core where the sender is run
    -r          identifier of the core where the receiver is run
    -S          carry out test B (if not set, test A is carried out)

Examples:

$ ./notify  # measure receiver side cost
001.630054 snd_body   [80] sender rate 3.514e+06 ops
001.630056 rcv_body   [140] receiver rate 2.971e+06 ops, #wakeups 1690750

    The output reports 2.97 millions of wait() operations, which correspond
    to 1.69 millions wake up. The sender in this case sent 2 millions
    notifications, therefore part of them (~300K) did not cause a receiver
    wake-up, because the receiver was already awake.

$ ./notify -S  # measure sender side cost
004.410564 rcv_body   [140] receiver rate 1.190e+04 ops, #wakeups 40000
004.410643 snd_slow_b [109] sender cost 2.601e+03 ns

    The output reports an average of 2.6 microseconds per notification,
    with 40000 notifications sent. Note that all the notifications turned
    out into a receiver wake-up.
